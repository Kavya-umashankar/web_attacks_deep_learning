{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    global sqlidf\n",
    "    contents=[]\n",
    "    with open(\"sqli.csv\",'r',encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.split('\\n')\n",
    "            list2 = [x for x in word if x]\n",
    "            list1 = list2[0].rsplit(',',maxsplit=1)\n",
    "            sentence=list1[0][1:]\n",
    "            label=list1[1][:-1]\n",
    "            listx=[sentence,label]\n",
    "            contents += [listx]\n",
    "\n",
    "    contents=contents[1:]\n",
    "    sqlidf = pd.DataFrame(contents,columns=['Sentence','Label'])\n",
    "    \n",
    "load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlidf['Sentence'] = sqlidf['Sentence'].astype(str)\n",
    "sqlidf['Label']=sqlidf['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sqlidf['Sentence']\n",
    "y=sqlidf['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(\"xtest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "posts = vectorizer.fit_transform(X_train).toarray()\n",
    "test_posts = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "(3360, 8781)\n"
     ]
    }
   ],
   "source": [
    "print(posts[5])\n",
    "print(posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vectorizer.vocabulary_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(20, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(10,  activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n"
     ]
    }
   ],
   "source": [
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                175640    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 192,235\n",
      "Trainable params: 190,187\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 2s 6ms/step - loss: 0.1954 - accuracy: 0.9176 - val_loss: 0.5181 - val_accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9676 - val_loss: 0.4440 - val_accuracy: 0.7262\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9768 - val_loss: 0.5143 - val_accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9759 - val_loss: 0.3223 - val_accuracy: 0.8143\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9783 - val_loss: 0.1239 - val_accuracy: 0.9619\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9798 - val_loss: 0.0896 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9798 - val_loss: 0.0602 - val_accuracy: 0.9774\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9801 - val_loss: 0.0619 - val_accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9780 - val_loss: 0.0509 - val_accuracy: 0.9762\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9801 - val_loss: 0.0476 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "classifier_nn = model.fit(posts,y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(test_posts, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer.vocabulary_, open(\"dictionary.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    if pred[i]>0.5:\n",
    "        pred[i]=1\n",
    "    elif pred[i]<=0.5:\n",
    "        pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(tp,tn,fp,fn):\n",
    "    \n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def precision_function(tp,fp):\n",
    "    \n",
    "    precision = tp / (tp+fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_function(tp,fn):\n",
    "    \n",
    "    recall=tp / (tp+fn)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "\n",
    "def confusion_matrix(truth,predicted):\n",
    "    \n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for true,pred in zip(truth,predicted):\n",
    "        if true == 1:\n",
    "            if pred == true:\n",
    "                true_positive += 1\n",
    "            elif pred != true:\n",
    "                false_negative += 1\n",
    "\n",
    "        elif true == 0:\n",
    "            if pred == true:\n",
    "                true_negative += 1\n",
    "            elif pred != true:\n",
    "                false_positive += 1\n",
    "            \n",
    "    accuracy=accuracy_function(true_positive, true_negative, false_positive, false_negative)\n",
    "    precision=precision_function(true_positive, false_positive)\n",
    "    recall=recall_function(true_positive, false_negative)\n",
    "    confusion_matrix_res = [[true_negative, false_negative],[false_positive,true_positive]]\n",
    "    \n",
    "    return (accuracy,\n",
    "            precision,\n",
    "           recall,\n",
    "           confusion_matrix_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy : 0.9773809523809524 \n",
      " Precision : 0.9953271028037384 \n",
      " Recall : 0.922077922077922 \n",
      " Confusion matrix: [[608, 18], [1, 213]]\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall, matrix =confusion_matrix(y_test,pred)\n",
    "print(\" Accuracy : {0} \\n Precision : {1} \\n Recall : {2} \\n Confusion matrix: {3}\".format(accuracy, precision, recall, matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(posts, y_train, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def testing1(querystring):\n",
    "    instance = X_test\n",
    "    instance.iloc[0] = querystring[0]\n",
    "    vocabulary_to_load = pickle.load(open(\"dictionary.pickle\", 'rb'))\n",
    "    loaded_vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary_to_load)\n",
    "    loaded_vectorizer._validate_vocabulary()\n",
    "    instance_posts = loaded_vectorizer.transform(instance).toarray()\n",
    "    \n",
    "    pred = loaded_model.predict(instance_posts)\n",
    "    \n",
    "    if pred[0]>0.5:\n",
    "        res=1\n",
    "    else:\n",
    "        res=0\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "time =  0.1441035270690918\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#hello world!\n",
    "start = time.time()\n",
    "print(testing1([\"105 OR 1=1\"]))\n",
    "stop = time.time()\n",
    "print(\"time = \",stop-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and evaluate a saved model\n",
    "# from numpy import loadtxt\n",
    "# from tensorflow.keras.models import load_model\n",
    " \n",
    "# # load model\n",
    "# model = load_model('model.h5')\n",
    "# # summarize model.\n",
    "# model.summary()\n",
    "# # load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
