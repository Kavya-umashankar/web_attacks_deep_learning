{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37115010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D,Flatten\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fa96b",
   "metadata": {},
   "source": [
    "## SQLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd958d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Users/kavya/AppData/Local/Programs/Python/Python38/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def load_csv():\n",
    "    global sqlidf\n",
    "    contents=[]\n",
    "    with open(\"sqli.csv\",'r',encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.split('\\n')\n",
    "            list2 = [x for x in word if x]\n",
    "            list1 = list2[0].rsplit(',',maxsplit=1)\n",
    "            sentence=list1[0][1:]\n",
    "            label=list1[1][:-1]\n",
    "            listx=[sentence,label]\n",
    "            contents += [listx]\n",
    "\n",
    "    contents=contents[1:]\n",
    "    sqlidf = pd.DataFrame(contents,columns=['Sentence','Label'])\n",
    "    \n",
    "load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543e791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlidf['Sentence'] = sqlidf['Sentence'].astype(str)\n",
    "sqlidf['Label']=sqlidf['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bd1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sqlidf['Sentence']\n",
    "y=sqlidf['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b80698",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "posts = vectorizer.fit_transform(X_train).toarray()\n",
    "test_posts = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe93944",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vectorizer.vocabulary_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(128,  activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3275841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1135616   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,160,449\n",
      "Trainable params: 1,160,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31fcc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 2s 8ms/step - loss: 0.2238 - accuracy: 0.8973 - val_loss: 0.0528 - val_accuracy: 0.9750\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 0.0357 - accuracy: 0.9801 - val_loss: 0.0414 - val_accuracy: 0.9774\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 0.0295 - accuracy: 0.9812 - val_loss: 0.0406 - val_accuracy: 0.9774\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.0291 - accuracy: 0.9824 - val_loss: 0.0410 - val_accuracy: 0.9774\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.0301 - accuracy: 0.9804 - val_loss: 0.0406 - val_accuracy: 0.9726\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.0281 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 0.9714\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 0.0273 - accuracy: 0.9827 - val_loss: 0.0402 - val_accuracy: 0.9774\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.0280 - accuracy: 0.9801 - val_loss: 0.0473 - val_accuracy: 0.9762\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 0.0265 - accuracy: 0.9824 - val_loss: 0.0403 - val_accuracy: 0.9774\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.0268 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "classifier_nn = model.fit(posts,y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(test_posts, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402c98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer.vocabulary_, open(\"dictionary.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6871fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3bf01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    if pred[i]>0.5:\n",
    "        pred[i]=1\n",
    "    elif pred[i]<=0.5:\n",
    "        pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f573695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(tp,tn,fp,fn):\n",
    "    \n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def precision_function(tp,fp):\n",
    "    \n",
    "    precision = tp / (tp+fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_function(tp,fn):\n",
    "    \n",
    "    recall=tp / (tp+fn)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "\n",
    "def confusion_matrix(truth,predicted):\n",
    "    \n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for true,pred in zip(truth,predicted):\n",
    "        if true == 1:\n",
    "            if pred == true:\n",
    "                true_positive += 1\n",
    "            elif pred != true:\n",
    "                false_negative += 1\n",
    "\n",
    "        elif true == 0:\n",
    "            if pred == true:\n",
    "                true_negative += 1\n",
    "            elif pred != true:\n",
    "                false_positive += 1\n",
    "            \n",
    "    accuracy=accuracy_function(true_positive, true_negative, false_positive, false_negative)\n",
    "    precision=precision_function(true_positive, false_positive)\n",
    "    recall=recall_function(true_positive, false_negative)\n",
    "    confusion_matrix_res = [[true_negative, false_negative],[false_positive,true_positive]]\n",
    "    \n",
    "    return (accuracy,\n",
    "            precision,\n",
    "           recall,\n",
    "           confusion_matrix_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2a09d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy : 0.9773809523809524 \n",
      " Precision : 0.9282868525896414 \n",
      " Recall : 0.9957264957264957 \n",
      " Confusion matrix: [[588, 1], [18, 233]]\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall, matrix =confusion_matrix(y_test,pred)\n",
    "print(\" Accuracy : {0} \\n Precision : {1} \\n Recall : {2} \\n Confusion matrix: {3}\".format(accuracy, precision, recall, matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96fa80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save(\"sqli_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe680f",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a8331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def testing1(querystring):\n",
    "    instance = X_test\n",
    "    instance.iloc[0] = querystring[0]\n",
    "    vocabulary_to_load = pickle.load(open(\"dictionary.pickle\", 'rb'))\n",
    "    loaded_vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary_to_load)\n",
    "    loaded_vectorizer._validate_vocabulary()\n",
    "    instance_posts = loaded_vectorizer.transform(instance).toarray()\n",
    "    \n",
    "    pred = model.predict(instance_posts)\n",
    "    \n",
    "    if pred[0]>0.5:\n",
    "        res=1\n",
    "    else:\n",
    "        res=0\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d14542d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "time =  0.1817927360534668\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#hello world!\n",
    "start = time.time()\n",
    "print(testing1([\"105 OR 1=1\"]))\n",
    "stop = time.time()\n",
    "print(\"time = \",stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4012ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1140352   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,165,185\n",
      "Trainable params: 1,165,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# summarize model.\n",
    "model.summary()\n",
    "# load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b9c21",
   "metadata": {},
   "source": [
    "## XSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da18d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    contents=[]\n",
    "    with open(\"XSS_dataset.csv\",'r') as f:\n",
    "        for line in f:\n",
    "            word = line.split('\\n')\n",
    "            sentence = word[0]\n",
    "            index , string = sentence.split(',',maxsplit=1)\n",
    "            sentence, label = string.rsplit(',',maxsplit=1)\n",
    "            #sentence = sentence.strip('\"')\n",
    "            contents += [[sentence , label]]\n",
    "\n",
    "\n",
    "    contents=contents[1:]\n",
    "    #print(contents)\n",
    "    global xssdf\n",
    "    xssdf = pd.DataFrame(contents,columns=['Sentence','Label'])\n",
    "    xssdf = xssdf.replace({'\\t': ''}, regex=True)\n",
    "    xssdf['Sentence'] = xssdf['Sentence'].astype(str)\n",
    "    xssdf['Label']=xssdf['Label'].astype(int)\n",
    "    \n",
    "    \n",
    "load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b09245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xssdf['Sentence']\n",
    "y = xssdf['Label'].values\n",
    "trainX, testX, trainY, testY = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50798691",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences=trainX.values\n",
    "test_sentences=testX.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a2276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ascii(sentence):\n",
    "    sentence_ascii=[]\n",
    "\n",
    "    for i in sentence:\n",
    "       \n",
    "        if(ord(i)<8222):      # ”  :  8221\n",
    "            \n",
    "            if(ord(i)==8217): # ’  :  8217 \n",
    "                sentence_ascii.append(134)\n",
    "            \n",
    "            \n",
    "            if(ord(i)==8221): # ”  :  8221 \"\"\n",
    "                sentence_ascii.append(129)\n",
    "                \n",
    "            if(ord(i)==8220): # “  :  8220\n",
    "                sentence_ascii.append(130)\n",
    "                \n",
    "                \n",
    "            if(ord(i)==8216): # ‘  :  8216\n",
    "                sentence_ascii.append(131)\n",
    "                \n",
    "            if(ord(i)==8217): # ’  :  8217\n",
    "                sentence_ascii.append(132)\n",
    "            \n",
    "            if(ord(i)==8211): # –  :  8211\n",
    "                sentence_ascii.append(133)\n",
    "                \n",
    "                \n",
    "            \"\"\"\n",
    "            If values less than 128 store them else discard them\n",
    "            \"\"\"\n",
    "            if (ord(i)<=128):\n",
    "                    sentence_ascii.append(ord(i))\n",
    "    \n",
    "            else:\n",
    "                    pass\n",
    "            \n",
    "\n",
    "    zer=np.zeros((10000))\n",
    "\n",
    "    for i in range(len(sentence_ascii)):\n",
    "        zer[i]=sentence_ascii[i]\n",
    "\n",
    "    zer.shape=(100, 100)\n",
    "\n",
    "    return zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "263892f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    arr=np.zeros((len(sentences),100,100))\n",
    " \n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        image=convert_to_ascii(sentences[i])\n",
    "\n",
    "        x=np.asarray(image,dtype='float')\n",
    "        image =  cv2.resize(x, dsize=(100,100), interpolation=cv2.INTER_CUBIC)\n",
    "        image/=128\n",
    "        arr[i]=image\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571c89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = preprocessing(train_sentences)\n",
    "test_arr = preprocessing(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc613590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape :  (10952, 100, 100, 1)\n",
      "Test data shape :  (2739, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "data = train_arr.reshape(train_arr.shape[0], 100, 100, 1)\n",
    "test_data = test_arr.reshape(test_arr.shape[0], 100, 100, 1)\n",
    "print(\"Train data shape : \",data.shape)\n",
    "print(\"Test data shape : \",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a159cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64,(3,3), activation=tf.nn.relu, input_shape=(100,100,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(256,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "781477c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25600)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               6553856   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,964,737\n",
      "Trainable params: 6,964,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6df59717",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 \n",
    "train_y = keras.utils.np_utils.to_categorical(data, num_classes)\n",
    "test_y = keras.utils.np_utils.to_categorical(test_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc0a9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "86/86 [==============================] - 30s 215ms/step - loss: 0.4138 - accuracy: 0.8193 - val_loss: 0.4758 - val_accuracy: 0.7996\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 17s 201ms/step - loss: 0.1554 - accuracy: 0.9456 - val_loss: 0.0725 - val_accuracy: 0.9810\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 17s 200ms/step - loss: 0.0720 - accuracy: 0.9804 - val_loss: 0.0548 - val_accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 17s 200ms/step - loss: 0.0557 - accuracy: 0.9852 - val_loss: 0.0582 - val_accuracy: 0.9843\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 17s 200ms/step - loss: 0.0503 - accuracy: 0.9868 - val_loss: 0.0490 - val_accuracy: 0.9876\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 17s 201ms/step - loss: 0.0451 - accuracy: 0.9884 - val_loss: 0.0454 - val_accuracy: 0.9854\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 17s 203ms/step - loss: 0.0393 - accuracy: 0.9891 - val_loss: 0.0496 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 17s 201ms/step - loss: 0.0358 - accuracy: 0.9895 - val_loss: 0.0381 - val_accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 17s 201ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.0464 - val_accuracy: 0.9847\n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 17s 201ms/step - loss: 0.0267 - accuracy: 0.9916 - val_loss: 0.0341 - val_accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epoch = 10\n",
    "\n",
    "#model training\n",
    "model_log = model.fit(data, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=( test_data,  testY)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d1d0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf4f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    if pred[i]>0.5:\n",
    "        pred[i]=1\n",
    "    elif pred[i]<=0.5:\n",
    "        pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2adcbd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy : 0.9905074844833881 \n",
      " Precision : 0.9911684782608695 \n",
      " Recall : 0.9911684782608695 \n",
      " Confusion matrix: [[1254, 13], [13, 1459]]\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall, matrix =confusion_matrix(testY,pred)\n",
    "print(\" Accuracy : {0} \\n Precision : {1} \\n Recall : {2} \\n Confusion matrix: {3}\".format(accuracy, precision, recall, matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4b873",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49c27aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(querystring):\n",
    "    instance=[]\n",
    "    instance = testX\n",
    "    instance = instance[:250]\n",
    "    instance[-1] = querystring[0]\n",
    "    test_instance=instance.values\n",
    "    instance_arr = preprocessing(test_instance)\n",
    "    instance_data = instance_arr.reshape(instance_arr.shape[0], 100, 100, 1)\n",
    "    pred=model.predict(instance_data)\n",
    "    if pred[-1]>0.5:\n",
    "            res=1\n",
    "    else:\n",
    "            res=0\n",
    "\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c0b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"xssmodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save(\"xssmodel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e388c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "testing(['architha aaa'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5068c7fa2bbab9f04583718d2e1425202789f781f9d0465b88a5638b9c49f2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
